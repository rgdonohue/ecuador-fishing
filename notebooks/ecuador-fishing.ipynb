{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ecuador Fishing Industry Data Scraper\n",
    "\n",
    "Objective: Script to scrape data about fishing vessels and fishmeal plants from Ecuador's government websites\n",
    "\n",
    "Sources: \n",
    "- https://srp.produccion.gob.ec/industrial/web/embarcaciones\n",
    "- https://bitacora.produccion.gob.ec/industrial/web/empresas\n",
    "\n",
    "To explore: \n",
    "https://datos.produccion.gob.ar/dataset/distribucion-geografica-de-los-establecimientos-productivos\n",
    "https://datos.produccion.gob.ar/dataset/distribucion-geografica-de-los-establecimientos-productivos/archivo/15d42a00-0d1f-480c-bea8-3257e34b7804\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base URL for the site\n",
    "vessel_base_url = \"https://bitacora.produccion.gob.ec/industrial/web/embarcaciones\"\n",
    "\n",
    "# Base URL for fishmeal plants\n",
    "fishmeal_base_url = \"https://bitacora.produccion.gob.ec/industrial/web/empresas\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape main page data\n",
    "def scrape_main_page(url, item_div_class, item_name_class, item_id_class):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    items = []\n",
    "    # Find all items\n",
    "    item_divs = soup.find_all('div', class_=item_div_class)\n",
    "    for item in item_divs:\n",
    "        name = item.find('div', class_=item_name_class).get_text(strip=True)\n",
    "        identifier = item.find('div', class_=item_id_class).get_text(strip=True)\n",
    "        details_link = item.find('a', string='Ver detalles')['href']\n",
    "        items.append({\n",
    "            'name': name,\n",
    "            'identifier': identifier,\n",
    "            'details_link': details_link\n",
    "        })\n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape vessel details page data\n",
    "def scrape_vessel_details_page(url):\n",
    "    url = 'https://srp.produccion.gob.ec' + url\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    details = {}\n",
    "\n",
    "    # Ensure there are at least three divs to avoid IndexError\n",
    "    divs = soup.find_all('div', class_='col-md-4')\n",
    "    if len(divs) < 3:\n",
    "        print(f\"Warning: Unexpected structure for details page: {url}\")\n",
    "        return details\n",
    "\n",
    "    # Extract alt text from the image in the first div\n",
    "    first_div = divs[0]\n",
    "    img_alt = first_div.find('img')['alt'] if first_div.find('img') else None\n",
    "    details['image_alt'] = img_alt\n",
    "\n",
    "    # Extract pairs of item-name and item-res from the second div\n",
    "    second_div = divs[1]\n",
    "    spans = second_div.find_all('span', class_=['item-name', 'item-res'])\n",
    "    for i in range(0, len(spans) - 1, 2):\n",
    "        if 'item-name' in spans[i].get('class', []) and 'item-res' in spans[i + 1].get('class', []):\n",
    "            key = spans[i].get_text(strip=True)\n",
    "            value = spans[i + 1].get_text(strip=True)\n",
    "            details[key] = value\n",
    "\n",
    "    # Extract key-value pairs from the third div\n",
    "    third_div = divs[2]\n",
    "    span_pairs = third_div.find_all('span', class_='item-res')\n",
    "    for i in range(0, len(span_pairs) - 1, 2):\n",
    "        key = span_pairs[i].get_text(strip=True)\n",
    "        value = span_pairs[i + 1].get_text(strip=True)\n",
    "        details[key] = value\n",
    "\n",
    "    return details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape fishmeal plant details page data\n",
    "def scrape_fishmeal_details_page(url):\n",
    "    url = 'https://bitacora.produccion.gob.ec' + url\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    details = {}\n",
    "\n",
    "    # Extract data from the first div (second part of the image name)\n",
    "    first_div = soup.find('div', class_='col-md-4')\n",
    "    img = first_div.find('img') if first_div else None\n",
    "    if img and 'src' in img.attrs:\n",
    "        img_name = img['src'].split('/')[-1].split('.')[0]  # Extract image name\n",
    "        if '-' in img_name:\n",
    "            key, value = img_name.split('-', 1)\n",
    "            details[key.capitalize()] = value.capitalize()\n",
    "\n",
    "    # Get all elements with class item-name\n",
    "    item_names = soup.find_all(class_='item-name')\n",
    "\n",
    "    for item_name in item_names:\n",
    "        # Get the heading text (column name)\n",
    "        heading = item_name.get_text(strip=True)\n",
    "\n",
    "        # Find the next sibling element with class item-res\n",
    "        item_res_elements = []\n",
    "        next_sibling = item_name.find_next_sibling(class_='item-res')\n",
    "\n",
    "        # Handle the special case for 'Acuerdo Ministerial'\n",
    "        if heading.startswith('Acuerdo Ministerial'):\n",
    "            while next_sibling and 'item-res' in next_sibling.get('class', []):\n",
    "                item_res_elements.append(next_sibling.get_text(strip=True))\n",
    "                next_sibling = next_sibling.find_next_sibling(class_='item-res')\n",
    "\n",
    "            details[heading] = item_res_elements if item_res_elements else 'N/A'\n",
    "        else:\n",
    "            # For other fields, use the text of the first 'item-res' sibling\n",
    "            details[heading] = next_sibling.get_text(strip=True) if next_sibling else 'N/A'\n",
    "\n",
    "    return details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape all pages\n",
    "def scrape_all_pages(base_url, item_div_class, item_name_class, item_id_class):\n",
    "    page = 1\n",
    "    all_items = []\n",
    "\n",
    "    while True:\n",
    "        print(f\"Scraping page {page}...\")\n",
    "        url = f\"{base_url}/index?page={page}&per-page=10\"\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        # Scrape items from the current page\n",
    "        items = scrape_main_page(url,  item_div_class, item_name_class, item_id_class)\n",
    "        if not items:\n",
    "            break  # Stop if no items found (end of pagination)\n",
    "\n",
    "        all_items.extend(items)\n",
    "\n",
    "        # Check if there is a next page\n",
    "        next_button = soup.find('li', class_='next')\n",
    "        if not next_button or not next_button.find('a', class_='page-link'):\n",
    "            break\n",
    "\n",
    "        page += 1\n",
    "\n",
    "    return all_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping vessels\n",
    "print(\"Scraping vessels...\")\n",
    "all_vessels = scrape_all_pages(vessel_base_url, 'embarcacion-item', 'item-name', 'item-matricula')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping fishmeal plants\n",
    "print(\"Scraping fishmeal plants...\")\n",
    "all_fishmeal = scrape_all_pages(fishmeal_base_url, 'empresa-item', 'item-name', 'item-cedula')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artesenials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping details for vessels\n",
    "for i, vessel in enumerate(all_vessels, start=1):\n",
    "    print(f\"Scraping details for vessel {i} of {len(all_vessels)}: {vessel['name']}\")\n",
    "    details_url = vessel['details_link']\n",
    "    try:\n",
    "        vessel_details = scrape_vessel_details_page(details_url)\n",
    "        vessel.update(vessel_details)\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping details for vessel {vessel['name']}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping details for fishmeal plants\n",
    "failed_plants = []\n",
    "for i, plant in enumerate(all_fishmeal, start=1):\n",
    "    print(f\"Scraping details for plant {i} of {len(all_fishmeal)}: {plant['name']}\")\n",
    "    details_url = plant['details_link']\n",
    "    try:\n",
    "        plant_details = scrape_fishmeal_details_page(details_url)\n",
    "        plant.update(plant_details)\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping details for plant {plant['name']}: {e}\")\n",
    "        failed_plants.append(plant['name'])\n",
    "\n",
    "# Write failed plants to log file\n",
    "if failed_plants:\n",
    "    with open('plants-log.txt', 'w') as f:\n",
    "        f.write('\\n'.join(failed_plants))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to DataFrame and save to CSV\n",
    "vessel_df = pd.DataFrame(all_vessels)\n",
    "vessel_df.to_csv('vessel_data.csv', index=False)\n",
    "print(\"Scraping complete. Data saved to 'vessel_data.csv'.\")\n",
    "\n",
    "fishmeal_df = pd.DataFrame(all_fishmeal)\n",
    "fishmeal_df.to_csv('fishmeal_data-test.csv', index=False)\n",
    "print(\"Scraping complete. Data saved to 'fishmeal_data.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export both dataframes to Excel file with separate sheets\n",
    "with pd.ExcelWriter('ecuador_fishing_data.xlsx') as writer:\n",
    "    vessel_df.to_excel(writer, sheet_name='Vessels', index=False)\n",
    "    fishmeal_df.to_excel(writer, sheet_name='Fishmeal Plants', index=False)\n",
    "print(\"Data exported to 'ecuador_fishing_data.xlsx'\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
